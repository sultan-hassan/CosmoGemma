{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "165c74cd-b85a-4845-838f-264c0eb9e54a",
   "metadata": {},
   "source": [
    "# **Evaluating CosmoGemma using LLMs as a judge**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fed2794-645c-4d90-af20-d651b9d22acf",
   "metadata": {},
   "source": [
    "### Read the testing sample and compare the reference and predicted answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f89d08-533a-4e05-abb5-94366b374766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('arxiv_astrophco_qa_pairs_2023_testing.json', 'r') as f:\n",
    "    test = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f487e16-4531-4c4c-84a0-d0b209bfd03d",
   "metadata": {},
   "source": [
    "### See some entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87fe23e9-24de-4edc-92af-be80395cf85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the typical age of large-scale stellar disks in S0 galaxies that reside in denser environments? \n",
      "\n",
      "Reference answer from llama3.1: older than 10 Gyr \n",
      "\n",
      "Predicted answer from CosmoGemma: 100--200 My. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 3\n",
    "print (\"Question:\", test[index]['Question'], \"\\n\")\n",
    "print (\"Reference answer from llama3.1:\", test[index]['REF_ANS'], \"\\n\")\n",
    "print (\"Predicted answer from CosmoGemma:\", test[index]['PRED_ANS'],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fd740b-978d-412e-b5cb-bedf813ddcdd",
   "metadata": {},
   "source": [
    "### Use langchain, and Ollama to run llama3.1:70b-instruct-q2_K (or any model, but accuracy can be sensitive to the model choice, requires extensive testing) model to grade CosmoGemma predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef3bc851-11e5-4c76-aa0e-d49e221dee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"task\", description=\"whether the two provided responses of a given question carry the exact meaning?\"),\n",
    "    ResponseSchema(name=\"output\", description=\"graded output either CORRECT or WRONG\")]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions(only_json=True)\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1:70b-instruct-q2_K\", temperature=0.0, format='json')#, keep_alive='16h', num_thread=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448b60e8-12fd-43e7-8a1c-421e203262ed",
   "metadata": {},
   "source": [
    "### Prompte template to generate QA pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d9a64bf-39ef-4cc9-bd76-417f0a5f0d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\" You are a cosmologist and judge. Your task is to grade whether the following two responses \"{ref_ans}\" and \"{pred_ans}\" of this given question \"{question}\" carry the exact meaning.\\n\\n\n",
    "\"You MUST obey the following criteria:\\n\"   \n",
    "\"- Just give either CORRECT or WRONG as a response, and no other detail or explanation. just one word response.\"\n",
    "\"- Please follow JSON recommended format below.\\n\" \n",
    "\"- Please ensure that the ouput is a valid JSON object.\\n\"                                                                                                                     \n",
    "\"{format_instructions}\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=TEMPLATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0880f71e-5648-469d-8406-7647c84ecdf4",
   "metadata": {},
   "source": [
    "### Loop through all answers to test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f66c5b9-4889-41ee-ac0b-d80e7c314db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation 0\n",
      "Task : whether the two provided responses of a given question carry the exact meaning? \n",
      "\n",
      "Reference Answer : Yes, they can, with accuracies comparable to or even better than those of the stage-III type surveys neglecting the effect of massive neutrinos. \n",
      "\n",
      "Predicted Answer : Yes, it can. \n",
      "\n",
      "Graded output : CORRECT \n",
      "\n",
      "Evaluation 1\n",
      "Task : Grading whether the two responses carry the exact meaning \n",
      "\n",
      "Reference Answer : Yes, they can contaminate the search for a primordial local signal by f_NL>10. \n",
      "\n",
      "Predicted Answer : yes \n",
      "\n",
      "Graded output : CORRECT \n",
      "\n",
      "Accuracy= 1.0\n",
      "Failed parsing 0\n"
     ]
    }
   ],
   "source": [
    "N = 0    \n",
    "failed = 0\n",
    "total = 2 #run few examples as a demo or use len(test)) to run through all the testing sample.\n",
    "for i in range(total): \n",
    "    print (\"Evaluation\", i)\n",
    "    messages = prompt.format_messages(ref_ans=test[i]['REF_ANS'],\n",
    "                                      pred_ans=test[i]['PRED_ANS'],\n",
    "                                      question=test[i]['Question'],\n",
    "                                      format_instructions=format_instructions) \n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    try:\n",
    "        output_dict = output_parser.parse(response.content)\n",
    "    \n",
    "        print (\"Task : \" + output_dict['task'], \"\\n\")\n",
    "        print (\"Reference Answer :\", test[i]['REF_ANS'],\"\\n\")\n",
    "        print (\"Predicted Answer :\", test[i]['PRED_ANS'],\"\\n\")\n",
    "    \n",
    "        print (\"Graded output :\", output_dict['output'],\"\\n\")\n",
    "        if output_dict['output'] == \"CORRECT\":\n",
    "            N+=1\n",
    "    except:\n",
    "        print (\"failed\", i)\n",
    "        failed+=1\n",
    "print (\"Accuracy=\",float(N/total))\n",
    "print (\"Failed parsing\",failed) #check how many runs failed during parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd5216-56ba-4a79-b2db-19994b6c5447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5d4eb-d204-4feb-84b2-07bbf0153981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c2183-b317-4020-8c39-8b01175c3d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
